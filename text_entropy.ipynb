{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is the measure of \"average surprisal\" per unit of information transmitted. (Or, in the case of storage, stored.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "READ_FROM_FILE = False\n",
    "if READ_FROM_FILE:\n",
    "    with open(input(\"Please enter the filename: \")) as f:\n",
    "        text = f.read()\n",
    "else:\n",
    "    text = input(\"Please enter the text, and press enter when you are finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHANUMERICS_ONLY = False\n",
    "# letter by letter\n",
    "if ALPHANUMERICS_ONLY:\n",
    "    text = ''.join([letter for letter in text if letter.isalnum()])\n",
    "set_of_letters = set(text)\n",
    "letters_entropy = 0.0\n",
    "for letter in set_of_letters:\n",
    "    freq = text.count(letter)/len(text)\n",
    "    letters_entropy += - freq*log(freq)\n",
    "\n",
    "theoretical_max_entropy = -log(1/len(set_of_letters)) # maximum entropy you can achieve using only this set of letters.\n",
    "print(\"The theoretical maximum entropy for this given length of string using this set of characters =\",\n",
    "      theoretical_max_entropy*len(text), \"nats, at \", theoretical_max_entropy, \"nats per character.\")\n",
    "\n",
    "print(\"The actual amount of entropy this message contains is \", letters_entropy*len(text), \"nats,\")\n",
    "print(\"which is equivalent to \", letters_entropy/log(2)*len(text), \"bits.\")\n",
    "print(\"The entropy per letter = \", letters_entropy, \"nats = \", letters_entropy/log(2), \"bits.\")\n",
    "print(\"Therefore transmitting the message character by character will take\", letters_entropy/log(2)*len(text),\n",
    "      \"bits even after compressing it optimally.\")\n",
    "\n",
    "print(\"A compression algorithm optimized to transmit this specific string can reduce the size of this string by\",\n",
    "      (1-(letters_entropy/theoretical_max_entropy))*100, \"% down to its minimum value stated above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word by word\n",
    "words = text.split()\n",
    "set_of_words = set(words)\n",
    "words_entropy = 0.0\n",
    "for word in set_of_words:\n",
    "    instance = len([match for match in text.split() if match==word])\n",
    "    freq = instance/len(text)\n",
    "    words_entropy += - freq*log(freq)\n",
    "\n",
    "theoretical_max_entropy = -log(1/len(set_of_words)) # maximum entropy you can achieve using only this set of words.\n",
    "print(\"The theoretical maximum entropy for this given length of string using this set of characters =\",\n",
    "      theoretical_max_entropy*len(text), \"nats, at \", theoretical_max_entropy, \"nats per character.\")\n",
    "\n",
    "print(\"The actual amount of entropy this message contains is \", words_entropy*len(text), \"nats,\")\n",
    "print(\"which is equivalent to \", words_entropy/log(2)*len(text), \"bits.\")\n",
    "print(\"The entropy per word = \", words_entropy, \"nats = \", words_entropy/log(2), \"bits.\")\n",
    "print(\"Therefore transmitting the message character by character will take\", words_entropy/log(2)*len(text),\n",
    "      \"bits even after compressing it optimally.\")\n",
    "\n",
    "print(\"A compression algorithm optimized to transmit this specific string can reduce the size of this string by\",\n",
    "      (1-(words_entropy/theoretical_max_entropy))*100, \"% down to its minimum value stated above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the cells above, entropy of a text can be measured in different ways: word by word, letter by letter, or even phoneme by phoneme.\n",
    "\n",
    "Measuring them letter by letter is equivalent to measuring the effort required to type this text out on a normal keyboard; while measuring them word by word is equivalent ot measuring the effort required to type it out on an enlarged keyboard, where each key corresponds to a word. The larger the keyboard, the harder it is to find the right key; but the fewer keystrokes are needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
